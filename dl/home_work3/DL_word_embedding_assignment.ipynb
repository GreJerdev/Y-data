{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding - Home Assigment\n",
    "## Dr. Omri Allouche 2018. YData Deep Learning Course\n",
    "\n",
    "[Open in Google Colab](https://colab.research.google.com/github/omriallouche/deep_learning_course/blob/master/DL_word_embedding_assignment.ipynb)\n",
    "    \n",
    "    \n",
    "In this exercise, you'll use word vectors trained on a corpus of 380,000 lyrics of songs from MetroLyrics (https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics).  \n",
    "The dataset contains these fields for each song, in CSV format:\n",
    "1. index\n",
    "1. song\n",
    "1. year\n",
    "1. artist\n",
    "1. genre\n",
    "1. lyrics\n",
    "\n",
    "Before doing this exercise, we recommend that you go over the \"Bag of words meets bag of popcorn\" tutorial (https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
    "\n",
    "Other recommended resources:\n",
    "- https://rare-technologies.com/word2vec-tutorial/\n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word vectors\n",
    "Train word vectors using the Skipgram Word2vec algorithm and the gensim package.\n",
    "Make sure you perform the following:\n",
    "- Tokenize words\n",
    "- Lowercase all words\n",
    "- Remove punctuation marks\n",
    "- Remove rare words\n",
    "- Remove stopwords\n",
    "\n",
    "Use 300 as the dimension of the word vectors. Try different context sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"380000-lyrics-from-metrolyrics.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['index', 'song', 'year', 'artist', 'genre', 'lyrics'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362237, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                    song  year           artist genre  \\\n",
      "0      0               ego-remix  2009  beyonce-knowles   Pop   \n",
      "1      1            then-tell-me  2009  beyonce-knowles   Pop   \n",
      "2      2                 honesty  2009  beyonce-knowles   Pop   \n",
      "3      3         you-are-my-rock  2009  beyonce-knowles   Pop   \n",
      "4      4           black-culture  2009  beyonce-knowles   Pop   \n",
      "5      5  all-i-could-do-was-cry  2009  beyonce-knowles   Pop   \n",
      "6      6      once-in-a-lifetime  2009  beyonce-knowles   Pop   \n",
      "7      7                 waiting  2009  beyonce-knowles   Pop   \n",
      "8      8               slow-love  2009  beyonce-knowles   Pop   \n",
      "9      9   why-don-t-you-love-me  2009  beyonce-knowles   Pop   \n",
      "\n",
      "                                              lyrics  \n",
      "0  Oh baby, how you doing?\\nYou know I'm gonna cu...  \n",
      "1  playin' everything so easy,\\nit's like you see...  \n",
      "2  If you search\\nFor tenderness\\nIt isn't hard t...  \n",
      "3  Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...  \n",
      "4  Party the people, the people the party it's po...  \n",
      "5  I heard\\nChurch bells ringing\\nI heard\\nA choi...  \n",
      "6  This is just another day that I would spend\\nW...  \n",
      "7  Waiting, waiting, waiting, waiting\\nWaiting, w...  \n",
      "8  [Verse 1:]\\nI read all of the magazines\\nwhile...  \n",
      "9  N-n-now, honey\\nYou better sit down and look a...  \n"
     ]
    }
   ],
   "source": [
    "print(raw_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock             131377\n",
       "Pop               49444\n",
       "Hip-Hop           33965\n",
       "Not Available     29814\n",
       "Metal             28408\n",
       "Other             23683\n",
       "Country           17286\n",
       "Jazz              17147\n",
       "Electronic        16205\n",
       "R&B                5935\n",
       "Indie              5732\n",
       "Folk               3241\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(362237, 6)\n",
      "(266505, 6)\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "data = raw_data.loc[raw_data[\"lyrics\"].str.len() > 3]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samuelguedj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def document_to_words(text, remove_panctuations=True, remove_stopwords=False):\n",
    "    if remove_panctuations:\n",
    "      text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]   \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'baby',\n",
       " 'know',\n",
       " 'gonna',\n",
       " 'cut',\n",
       " 'right',\n",
       " 'chase',\n",
       " 'women',\n",
       " 'made',\n",
       " 'like']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_to_words(data[\"lyrics\"][0], remove_stopwords=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samuelguedj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oh baby, how you doing?',\n",
       " \"You know I'm gonna cut right to the chase\\nSome women were made but me, myself\\nI like to think that I was created for a special purpose\\nYou know, what's more special than you?\",\n",
       " \"You feel me\\nIt's on baby, let's get lost\\nYou don't need to call into work 'cause you're the boss\\nFor real, want you to show me how you feel\\nI consider myself lucky, that's a big deal\\nWhy?\",\n",
       " \"Well, you got the key to my heart\\nBut you ain't gonna need it, I'd rather you open up my body\\nAnd show me secrets, you didn't know was inside\\nNo need for me to lie\\nIt's too big, it's too wide\\nIt's too strong, it won't fit\\nIt's too much, it's too tough\\nHe talk like this 'cause he can back it up\\nHe got a big ego, such a huge ego\\nI love his big ego, it's too much\\nHe walk like this 'cause he can back it up\\nUsually I'm humble, right now I don't choose\\nYou can leave with me or you could have the blues\\nSome call it arrogant, I call it confident\\nYou decide when you find on what I'm working with\\nDamn I know I'm killing you with them legs\\nBetter yet them thighs\\nMatter a fact it's my smile or maybe my eyes\\nBoy you a site to see, kind of something like me\\nIt's too big, it's too wide\\nIt's too strong, it won't fit\\nIt's too much, it's too tough\\nI talk like this 'cause I can back it up\\nI got a big ego, such a huge ego\\nBut he love my big ego, it's too much\\nI walk like this 'cause I can back it up\\nI, I walk like this 'cause I can back it up\\nI, I talk like this 'cause I can back it up\\nI, I can back it up, I can back it up\\nI walk like this 'cause I can back it up\\nIt's too big, it's too wide\\nIt's too strong, it won't fit\\nIt's too much, it's too tough\\nHe talk like this 'cause he can back it up\\nHe got a big ego, such a huge ego, such a huge ego\\nI love his big ego, it's too much\\nHe walk like this 'cause he can back it up\\nEgo so big, you must admit\\nI got every reason to feel like I'm that bitch\\nEgo so strong, if you ain't know\\nI don't need no beat, I can sing it with piano\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(data[\"lyrics\"][0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_sentences(document, tokenizer, remove_stopwords=False):\n",
    "    sentences = []\n",
    "    if tokenizer:\n",
    "        raw_sentences = tokenizer.tokenize(document)\n",
    "    else:\n",
    "        raw_sentences = document\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(document_to_words(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = document_to_sentences(data[\"lyrics\"][0], tokenizer, remove_stopwords=True)\n",
    "len(sentence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for song_lyrics in data[\"lyrics\"]:\n",
    "    sentences += document_to_sentences(song_lyrics, tokenizer, \n",
    "                                       remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec_hat(sentences, num_features=300, min_word_count=40, workers=4, \n",
    "             context=10, downsampling=1e-3, save_model=True):\n",
    "\n",
    "    model = Word2Vec(sentences, workers=workers, size=num_features, \n",
    "                   min_count=min_word_count, window=context, sample=downsampling)\n",
    "\n",
    "    if save_model:\n",
    "        # If you don't plan to train the model any further, calling \n",
    "        # init_sims will make the model much more memory-efficient.\n",
    "#         model.init_sims(replace=True)\n",
    "        model.wv.init_sims\n",
    "        # It can be helpful to create a meaningful model name and \n",
    "        # save the model for later use. You can load it later using Word2Vec.load()\n",
    "        model_name = \"{}features_{}minwords_{}context.wv.model\".format(num_features, \n",
    "                                                              min_word_count, \n",
    "                                                              context)\n",
    "        model.save(model_name)\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:38:52: collecting all words and their counts\n",
      "INFO - 16:38:52: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:38:52: PROGRESS: at sentence #10000, processed 547073 words, keeping 24856 word types\n",
      "INFO - 16:38:52: PROGRESS: at sentence #20000, processed 1002175 words, keeping 36701 word types\n",
      "INFO - 16:38:52: PROGRESS: at sentence #30000, processed 1577546 words, keeping 45437 word types\n",
      "INFO - 16:38:53: PROGRESS: at sentence #40000, processed 2132629 words, keeping 55005 word types\n",
      "INFO - 16:38:53: PROGRESS: at sentence #50000, processed 2565209 words, keeping 60035 word types\n",
      "INFO - 16:38:53: PROGRESS: at sentence #60000, processed 3125605 words, keeping 70378 word types\n",
      "INFO - 16:38:53: PROGRESS: at sentence #70000, processed 3650087 words, keeping 80685 word types\n",
      "INFO - 16:38:53: PROGRESS: at sentence #80000, processed 4152681 words, keeping 88085 word types\n",
      "INFO - 16:38:53: PROGRESS: at sentence #90000, processed 4712834 words, keeping 92527 word types\n",
      "INFO - 16:38:53: PROGRESS: at sentence #100000, processed 5346594 words, keeping 95618 word types\n",
      "INFO - 16:38:53: PROGRESS: at sentence #110000, processed 5804237 words, keeping 97869 word types\n",
      "INFO - 16:38:54: PROGRESS: at sentence #120000, processed 6238748 words, keeping 101497 word types\n",
      "INFO - 16:38:54: PROGRESS: at sentence #130000, processed 6747822 words, keeping 106167 word types\n",
      "INFO - 16:38:54: PROGRESS: at sentence #140000, processed 7264439 words, keeping 110304 word types\n",
      "INFO - 16:38:54: PROGRESS: at sentence #150000, processed 7826998 words, keeping 113139 word types\n",
      "INFO - 16:38:54: PROGRESS: at sentence #160000, processed 8341793 words, keeping 119257 word types\n",
      "INFO - 16:38:54: PROGRESS: at sentence #170000, processed 8873575 words, keeping 123630 word types\n",
      "INFO - 16:38:54: PROGRESS: at sentence #180000, processed 9412714 words, keeping 129259 word types\n",
      "INFO - 16:38:55: PROGRESS: at sentence #190000, processed 9966143 words, keeping 135923 word types\n",
      "INFO - 16:38:55: PROGRESS: at sentence #200000, processed 10526414 words, keeping 141990 word types\n",
      "INFO - 16:38:55: PROGRESS: at sentence #210000, processed 10982261 words, keeping 144912 word types\n",
      "INFO - 16:38:55: PROGRESS: at sentence #220000, processed 11367169 words, keeping 149330 word types\n",
      "INFO - 16:38:55: PROGRESS: at sentence #230000, processed 11988166 words, keeping 153274 word types\n",
      "INFO - 16:38:55: PROGRESS: at sentence #240000, processed 12532308 words, keeping 156680 word types\n",
      "INFO - 16:38:56: PROGRESS: at sentence #250000, processed 13079502 words, keeping 159781 word types\n",
      "INFO - 16:38:56: PROGRESS: at sentence #260000, processed 13567570 words, keeping 162857 word types\n",
      "INFO - 16:38:56: PROGRESS: at sentence #270000, processed 14076921 words, keeping 168418 word types\n",
      "INFO - 16:38:56: PROGRESS: at sentence #280000, processed 14645191 words, keeping 171879 word types\n",
      "INFO - 16:38:56: PROGRESS: at sentence #290000, processed 15200162 words, keeping 177206 word types\n",
      "INFO - 16:38:56: PROGRESS: at sentence #300000, processed 15647288 words, keeping 179877 word types\n",
      "INFO - 16:38:56: PROGRESS: at sentence #310000, processed 16214258 words, keeping 182211 word types\n",
      "INFO - 16:38:56: PROGRESS: at sentence #320000, processed 16785917 words, keeping 186105 word types\n",
      "INFO - 16:38:57: PROGRESS: at sentence #330000, processed 17218338 words, keeping 188173 word types\n",
      "INFO - 16:38:57: PROGRESS: at sentence #340000, processed 17774404 words, keeping 192031 word types\n",
      "INFO - 16:38:57: PROGRESS: at sentence #350000, processed 18370845 words, keeping 195242 word types\n",
      "INFO - 16:38:57: PROGRESS: at sentence #360000, processed 18946568 words, keeping 197477 word types\n",
      "INFO - 16:38:57: PROGRESS: at sentence #370000, processed 19440294 words, keeping 201296 word types\n",
      "INFO - 16:38:57: PROGRESS: at sentence #380000, processed 19925594 words, keeping 202681 word types\n",
      "INFO - 16:38:57: PROGRESS: at sentence #390000, processed 20452587 words, keeping 208062 word types\n",
      "INFO - 16:38:57: PROGRESS: at sentence #400000, processed 20844516 words, keeping 211371 word types\n",
      "INFO - 16:38:57: PROGRESS: at sentence #410000, processed 21428945 words, keeping 214812 word types\n",
      "INFO - 16:38:58: PROGRESS: at sentence #420000, processed 21881705 words, keeping 219208 word types\n",
      "INFO - 16:38:58: PROGRESS: at sentence #430000, processed 22462819 words, keeping 220763 word types\n",
      "INFO - 16:38:58: PROGRESS: at sentence #440000, processed 23066330 words, keeping 223281 word types\n",
      "INFO - 16:38:58: PROGRESS: at sentence #450000, processed 23662350 words, keeping 225996 word types\n",
      "INFO - 16:38:58: PROGRESS: at sentence #460000, processed 24224611 words, keeping 232691 word types\n",
      "INFO - 16:38:58: PROGRESS: at sentence #470000, processed 24849515 words, keeping 236393 word types\n",
      "INFO - 16:38:58: PROGRESS: at sentence #480000, processed 25402691 words, keeping 239509 word types\n",
      "INFO - 16:38:59: PROGRESS: at sentence #490000, processed 25896125 words, keeping 242861 word types\n",
      "INFO - 16:38:59: PROGRESS: at sentence #500000, processed 26428231 words, keeping 244691 word types\n",
      "INFO - 16:38:59: PROGRESS: at sentence #510000, processed 26955817 words, keeping 247883 word types\n",
      "INFO - 16:38:59: PROGRESS: at sentence #520000, processed 27487614 words, keeping 249512 word types\n",
      "INFO - 16:38:59: PROGRESS: at sentence #530000, processed 27971469 words, keeping 251533 word types\n",
      "INFO - 16:38:59: PROGRESS: at sentence #540000, processed 28490015 words, keeping 252916 word types\n",
      "INFO - 16:38:59: PROGRESS: at sentence #550000, processed 28989811 words, keeping 254959 word types\n",
      "INFO - 16:38:59: PROGRESS: at sentence #560000, processed 29475702 words, keeping 257380 word types\n",
      "INFO - 16:38:59: PROGRESS: at sentence #570000, processed 29924960 words, keeping 259980 word types\n",
      "INFO - 16:39:00: PROGRESS: at sentence #580000, processed 30435339 words, keeping 262416 word types\n",
      "INFO - 16:39:00: PROGRESS: at sentence #590000, processed 30986320 words, keeping 264861 word types\n",
      "INFO - 16:39:00: PROGRESS: at sentence #600000, processed 31561787 words, keeping 268688 word types\n",
      "INFO - 16:39:00: PROGRESS: at sentence #610000, processed 32112877 words, keeping 272019 word types\n",
      "INFO - 16:39:00: PROGRESS: at sentence #620000, processed 32642375 words, keeping 274986 word types\n",
      "INFO - 16:39:00: PROGRESS: at sentence #630000, processed 33062014 words, keeping 277041 word types\n",
      "INFO - 16:39:00: PROGRESS: at sentence #640000, processed 33543969 words, keeping 279806 word types\n",
      "INFO - 16:39:00: PROGRESS: at sentence #650000, processed 34230688 words, keeping 282203 word types\n",
      "INFO - 16:39:01: PROGRESS: at sentence #660000, processed 34791272 words, keeping 285091 word types\n",
      "INFO - 16:39:01: PROGRESS: at sentence #670000, processed 35309627 words, keeping 286699 word types\n",
      "INFO - 16:39:01: PROGRESS: at sentence #680000, processed 35812309 words, keeping 287788 word types\n",
      "INFO - 16:39:01: PROGRESS: at sentence #690000, processed 36402616 words, keeping 290155 word types\n",
      "INFO - 16:39:01: PROGRESS: at sentence #700000, processed 36931058 words, keeping 292733 word types\n",
      "INFO - 16:39:01: PROGRESS: at sentence #710000, processed 37459344 words, keeping 294911 word types\n",
      "INFO - 16:39:01: PROGRESS: at sentence #720000, processed 38035701 words, keeping 298075 word types\n",
      "INFO - 16:39:02: PROGRESS: at sentence #730000, processed 38526517 words, keeping 300419 word types\n",
      "INFO - 16:39:02: PROGRESS: at sentence #740000, processed 39078186 words, keeping 301908 word types\n",
      "INFO - 16:39:02: PROGRESS: at sentence #750000, processed 39602798 words, keeping 304326 word types\n",
      "INFO - 16:39:02: PROGRESS: at sentence #760000, processed 40204170 words, keeping 306727 word types\n",
      "INFO - 16:39:02: PROGRESS: at sentence #770000, processed 40727304 words, keeping 308543 word types\n",
      "INFO - 16:39:02: PROGRESS: at sentence #780000, processed 41308108 words, keeping 311788 word types\n",
      "INFO - 16:39:02: PROGRESS: at sentence #790000, processed 41840703 words, keeping 314452 word types\n",
      "INFO - 16:39:03: PROGRESS: at sentence #800000, processed 42396409 words, keeping 317538 word types\n",
      "INFO - 16:39:03: PROGRESS: at sentence #810000, processed 42881628 words, keeping 321897 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:39:03: PROGRESS: at sentence #820000, processed 43471094 words, keeping 323912 word types\n",
      "INFO - 16:39:03: PROGRESS: at sentence #830000, processed 44172564 words, keeping 325593 word types\n",
      "INFO - 16:39:03: PROGRESS: at sentence #840000, processed 44667279 words, keeping 330357 word types\n",
      "INFO - 16:39:03: PROGRESS: at sentence #850000, processed 45114992 words, keeping 331606 word types\n",
      "INFO - 16:39:03: PROGRESS: at sentence #860000, processed 45840735 words, keeping 334894 word types\n",
      "INFO - 16:39:04: PROGRESS: at sentence #870000, processed 46364336 words, keeping 338759 word types\n",
      "INFO - 16:39:04: PROGRESS: at sentence #880000, processed 46858701 words, keeping 340849 word types\n",
      "INFO - 16:39:04: PROGRESS: at sentence #890000, processed 47371857 words, keeping 345358 word types\n",
      "INFO - 16:39:04: PROGRESS: at sentence #900000, processed 47954498 words, keeping 346951 word types\n",
      "INFO - 16:39:04: PROGRESS: at sentence #910000, processed 48459183 words, keeping 351406 word types\n",
      "INFO - 16:39:04: PROGRESS: at sentence #920000, processed 48891710 words, keeping 353783 word types\n",
      "INFO - 16:39:04: PROGRESS: at sentence #930000, processed 49400367 words, keeping 355945 word types\n",
      "INFO - 16:39:04: PROGRESS: at sentence #940000, processed 49930820 words, keeping 357697 word types\n",
      "INFO - 16:39:05: PROGRESS: at sentence #950000, processed 50289438 words, keeping 358285 word types\n",
      "INFO - 16:39:05: PROGRESS: at sentence #960000, processed 50841972 words, keeping 359892 word types\n",
      "INFO - 16:39:05: PROGRESS: at sentence #970000, processed 51449504 words, keeping 362113 word types\n",
      "INFO - 16:39:05: PROGRESS: at sentence #980000, processed 51944940 words, keeping 365262 word types\n",
      "INFO - 16:39:05: PROGRESS: at sentence #990000, processed 52419089 words, keeping 367630 word types\n",
      "INFO - 16:39:05: PROGRESS: at sentence #1000000, processed 52917528 words, keeping 369143 word types\n",
      "INFO - 16:39:05: PROGRESS: at sentence #1010000, processed 53450021 words, keeping 371849 word types\n",
      "INFO - 16:39:06: PROGRESS: at sentence #1020000, processed 54024823 words, keeping 374148 word types\n",
      "INFO - 16:39:06: PROGRESS: at sentence #1030000, processed 54713585 words, keeping 375960 word types\n",
      "INFO - 16:39:06: PROGRESS: at sentence #1040000, processed 55288244 words, keeping 377553 word types\n",
      "INFO - 16:39:06: PROGRESS: at sentence #1050000, processed 55911724 words, keeping 379082 word types\n",
      "INFO - 16:39:07: PROGRESS: at sentence #1060000, processed 56549332 words, keeping 382291 word types\n",
      "INFO - 16:39:07: PROGRESS: at sentence #1070000, processed 56868533 words, keeping 383411 word types\n",
      "INFO - 16:39:07: PROGRESS: at sentence #1080000, processed 57449247 words, keeping 387694 word types\n",
      "INFO - 16:39:07: PROGRESS: at sentence #1090000, processed 58059497 words, keeping 389822 word types\n",
      "INFO - 16:39:07: PROGRESS: at sentence #1100000, processed 58470622 words, keeping 393227 word types\n",
      "INFO - 16:39:07: PROGRESS: at sentence #1110000, processed 58993729 words, keeping 395024 word types\n",
      "INFO - 16:39:07: PROGRESS: at sentence #1120000, processed 59292418 words, keeping 396371 word types\n",
      "INFO - 16:39:08: PROGRESS: at sentence #1130000, processed 59812235 words, keeping 398692 word types\n",
      "INFO - 16:39:08: PROGRESS: at sentence #1140000, processed 60462312 words, keeping 402023 word types\n",
      "INFO - 16:39:08: PROGRESS: at sentence #1150000, processed 60957239 words, keeping 404330 word types\n",
      "INFO - 16:39:08: PROGRESS: at sentence #1160000, processed 61457197 words, keeping 405916 word types\n",
      "INFO - 16:39:08: PROGRESS: at sentence #1170000, processed 62011934 words, keeping 408061 word types\n",
      "INFO - 16:39:08: PROGRESS: at sentence #1180000, processed 62484493 words, keeping 410107 word types\n",
      "INFO - 16:39:08: PROGRESS: at sentence #1190000, processed 63056200 words, keeping 412233 word types\n",
      "INFO - 16:39:09: collected 414720 word types from a corpus of 63696185 raw words and 1199770 sentences\n",
      "INFO - 16:39:09: Loading a fresh vocabulary\n",
      "INFO - 16:39:09: effective_min_count=40 retains 29972 unique words (7% of original 414720, drops 384748)\n",
      "INFO - 16:39:09: effective_min_count=40 leaves 62171067 word corpus (97% of original 63696185, drops 1525118)\n",
      "INFO - 16:39:09: deleting the raw counts dictionary of 414720 items\n",
      "INFO - 16:39:09: sample=0.001 downsamples 61 most-common words\n",
      "INFO - 16:39:09: downsampling leaves estimated 45784945 word corpus (73.6% of prior 62171067)\n",
      "INFO - 16:39:09: estimated required memory for 29972 words and 300 dimensions: 86918800 bytes\n",
      "INFO - 16:39:09: resetting layer weights\n",
      "INFO - 16:39:10: training model with 4 workers on 29972 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO - 16:39:11: EPOCH 1 - PROGRESS: at 2.04% examples, 895957 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:12: EPOCH 1 - PROGRESS: at 4.13% examples, 904612 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:13: EPOCH 1 - PROGRESS: at 5.65% examples, 845942 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:14: EPOCH 1 - PROGRESS: at 7.54% examples, 846866 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:15: EPOCH 1 - PROGRESS: at 9.81% examples, 875681 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:16: EPOCH 1 - PROGRESS: at 12.06% examples, 901867 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:17: EPOCH 1 - PROGRESS: at 14.42% examples, 923211 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:18: EPOCH 1 - PROGRESS: at 16.59% examples, 935347 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:19: EPOCH 1 - PROGRESS: at 19.11% examples, 947368 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:20: EPOCH 1 - PROGRESS: at 20.72% examples, 928707 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:21: EPOCH 1 - PROGRESS: at 22.88% examples, 928739 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:22: EPOCH 1 - PROGRESS: at 24.68% examples, 913931 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:23: EPOCH 1 - PROGRESS: at 26.11% examples, 896809 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:24: EPOCH 1 - PROGRESS: at 28.28% examples, 902700 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:25: EPOCH 1 - PROGRESS: at 30.39% examples, 911173 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:26: EPOCH 1 - PROGRESS: at 32.68% examples, 919249 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:27: EPOCH 1 - PROGRESS: at 35.14% examples, 923215 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:28: EPOCH 1 - PROGRESS: at 37.06% examples, 926577 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:29: EPOCH 1 - PROGRESS: at 38.60% examples, 919068 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:30: EPOCH 1 - PROGRESS: at 40.79% examples, 923530 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:31: EPOCH 1 - PROGRESS: at 43.04% examples, 928341 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:32: EPOCH 1 - PROGRESS: at 45.48% examples, 933155 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:33: EPOCH 1 - PROGRESS: at 47.81% examples, 934241 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:34: EPOCH 1 - PROGRESS: at 49.46% examples, 929339 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:35: EPOCH 1 - PROGRESS: at 51.60% examples, 931826 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:36: EPOCH 1 - PROGRESS: at 53.86% examples, 935963 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:37: EPOCH 1 - PROGRESS: at 56.21% examples, 940448 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:38: EPOCH 1 - PROGRESS: at 58.43% examples, 944383 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:39: EPOCH 1 - PROGRESS: at 60.65% examples, 947449 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:40: EPOCH 1 - PROGRESS: at 62.95% examples, 950729 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:41: EPOCH 1 - PROGRESS: at 65.04% examples, 953139 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:42: EPOCH 1 - PROGRESS: at 67.12% examples, 953612 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:43: EPOCH 1 - PROGRESS: at 68.98% examples, 954455 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:44: EPOCH 1 - PROGRESS: at 71.17% examples, 954981 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:45: EPOCH 1 - PROGRESS: at 72.53% examples, 943293 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:46: EPOCH 1 - PROGRESS: at 73.41% examples, 927564 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:47: EPOCH 1 - PROGRESS: at 75.16% examples, 924624 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:39:48: EPOCH 1 - PROGRESS: at 77.49% examples, 925531 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:49: EPOCH 1 - PROGRESS: at 79.94% examples, 927210 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:50: EPOCH 1 - PROGRESS: at 81.69% examples, 924593 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:51: EPOCH 1 - PROGRESS: at 83.07% examples, 917176 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:52: EPOCH 1 - PROGRESS: at 84.95% examples, 915031 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:53: EPOCH 1 - PROGRESS: at 86.65% examples, 915373 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:54: EPOCH 1 - PROGRESS: at 88.61% examples, 917594 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:55: EPOCH 1 - PROGRESS: at 90.94% examples, 919615 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:56: EPOCH 1 - PROGRESS: at 93.70% examples, 921703 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:57: EPOCH 1 - PROGRESS: at 95.73% examples, 923483 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:58: EPOCH 1 - PROGRESS: at 97.98% examples, 924493 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:59: EPOCH 1 - PROGRESS: at 99.78% examples, 923833 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:39:59: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:39:59: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:39:59: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:39:59: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:39:59: EPOCH - 1 : training on 63696185 raw words (45786038 effective words) took 49.5s, 924154 effective words/s\n",
      "INFO - 16:40:00: EPOCH 2 - PROGRESS: at 2.24% examples, 998742 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:01: EPOCH 2 - PROGRESS: at 4.44% examples, 997395 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:02: EPOCH 2 - PROGRESS: at 6.74% examples, 999629 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:03: EPOCH 2 - PROGRESS: at 8.61% examples, 990240 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:04: EPOCH 2 - PROGRESS: at 11.08% examples, 992025 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:05: EPOCH 2 - PROGRESS: at 13.16% examples, 981975 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:06: EPOCH 2 - PROGRESS: at 15.26% examples, 981619 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:07: EPOCH 2 - PROGRESS: at 17.62% examples, 985905 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:08: EPOCH 2 - PROGRESS: at 19.90% examples, 989647 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:09: EPOCH 2 - PROGRESS: at 22.21% examples, 992843 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:10: EPOCH 2 - PROGRESS: at 24.44% examples, 991737 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:11: EPOCH 2 - PROGRESS: at 26.48% examples, 990391 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:12: EPOCH 2 - PROGRESS: at 28.65% examples, 989364 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:13: EPOCH 2 - PROGRESS: at 30.75% examples, 990883 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:14: EPOCH 2 - PROGRESS: at 33.24% examples, 990418 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:15: EPOCH 2 - PROGRESS: at 35.37% examples, 989579 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:16: EPOCH 2 - PROGRESS: at 37.08% examples, 983796 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:17: EPOCH 2 - PROGRESS: at 38.78% examples, 977486 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:18: EPOCH 2 - PROGRESS: at 40.98% examples, 979969 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:19: EPOCH 2 - PROGRESS: at 43.28% examples, 982733 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:20: EPOCH 2 - PROGRESS: at 45.74% examples, 985936 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:21: EPOCH 2 - PROGRESS: at 48.20% examples, 988715 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:22: EPOCH 2 - PROGRESS: at 50.44% examples, 991833 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:23: EPOCH 2 - PROGRESS: at 52.71% examples, 991325 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:24: EPOCH 2 - PROGRESS: at 54.11% examples, 979119 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:25: EPOCH 2 - PROGRESS: at 56.03% examples, 975596 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:26: EPOCH 2 - PROGRESS: at 57.92% examples, 971726 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:27: EPOCH 2 - PROGRESS: at 59.92% examples, 971310 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:28: EPOCH 2 - PROGRESS: at 62.08% examples, 972365 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:29: EPOCH 2 - PROGRESS: at 63.94% examples, 970022 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:30: EPOCH 2 - PROGRESS: at 66.08% examples, 970145 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:31: EPOCH 2 - PROGRESS: at 67.92% examples, 966282 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:33: EPOCH 2 - PROGRESS: at 69.37% examples, 947866 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 16:40:34: EPOCH 2 - PROGRESS: at 71.11% examples, 942419 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:35: EPOCH 2 - PROGRESS: at 72.92% examples, 940660 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:36: EPOCH 2 - PROGRESS: at 74.86% examples, 939786 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:37: EPOCH 2 - PROGRESS: at 77.05% examples, 939202 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:38: EPOCH 2 - PROGRESS: at 78.20% examples, 927339 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:39: EPOCH 2 - PROGRESS: at 80.46% examples, 927008 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:40: EPOCH 2 - PROGRESS: at 81.30% examples, 915497 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:41: EPOCH 2 - PROGRESS: at 83.37% examples, 914224 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:42: EPOCH 2 - PROGRESS: at 85.45% examples, 916943 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:43: EPOCH 2 - PROGRESS: at 87.27% examples, 918622 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:44: EPOCH 2 - PROGRESS: at 89.71% examples, 920947 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:45: EPOCH 2 - PROGRESS: at 91.89% examples, 923118 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:46: EPOCH 2 - PROGRESS: at 94.21% examples, 921807 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:47: EPOCH 2 - PROGRESS: at 96.13% examples, 922544 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:48: EPOCH 2 - PROGRESS: at 97.75% examples, 917762 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:49: EPOCH 2 - PROGRESS: at 99.72% examples, 918477 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:49: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:40:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:40:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:40:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:40:49: EPOCH - 2 : training on 63696185 raw words (45782337 effective words) took 49.8s, 918712 effective words/s\n",
      "INFO - 16:40:50: EPOCH 3 - PROGRESS: at 2.29% examples, 1026876 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:51: EPOCH 3 - PROGRESS: at 4.55% examples, 1010986 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:52: EPOCH 3 - PROGRESS: at 6.24% examples, 921845 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:53: EPOCH 3 - PROGRESS: at 7.86% examples, 889060 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:54: EPOCH 3 - PROGRESS: at 9.35% examples, 840636 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:55: EPOCH 3 - PROGRESS: at 11.20% examples, 837440 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:56: EPOCH 3 - PROGRESS: at 12.88% examples, 819682 words/s, in_qsize 5, out_qsize 3\n",
      "INFO - 16:40:57: EPOCH 3 - PROGRESS: at 13.98% examples, 780742 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:40:58: EPOCH 3 - PROGRESS: at 14.98% examples, 742307 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 16:40:59: EPOCH 3 - PROGRESS: at 16.75% examples, 752287 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:00: EPOCH 3 - PROGRESS: at 18.96% examples, 763408 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:01: EPOCH 3 - PROGRESS: at 20.67% examples, 768286 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 16:41:02: EPOCH 3 - PROGRESS: at 22.11% examples, 741735 words/s, in_qsize 5, out_qsize 2\n",
      "INFO - 16:41:03: EPOCH 3 - PROGRESS: at 24.08% examples, 754255 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:04: EPOCH 3 - PROGRESS: at 26.39% examples, 771856 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:41:05: EPOCH 3 - PROGRESS: at 28.29% examples, 776426 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:06: EPOCH 3 - PROGRESS: at 30.05% examples, 782419 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:07: EPOCH 3 - PROGRESS: at 32.33% examples, 796158 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:08: EPOCH 3 - PROGRESS: at 34.62% examples, 806195 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:10: EPOCH 3 - PROGRESS: at 36.84% examples, 816285 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:11: EPOCH 3 - PROGRESS: at 38.78% examples, 824981 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:12: EPOCH 3 - PROGRESS: at 40.94% examples, 833273 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:13: EPOCH 3 - PROGRESS: at 43.28% examples, 842489 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:14: EPOCH 3 - PROGRESS: at 45.70% examples, 849640 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:15: EPOCH 3 - PROGRESS: at 47.98% examples, 853972 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:16: EPOCH 3 - PROGRESS: at 49.90% examples, 856342 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:17: EPOCH 3 - PROGRESS: at 52.03% examples, 861409 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:18: EPOCH 3 - PROGRESS: at 54.32% examples, 866186 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:19: EPOCH 3 - PROGRESS: at 56.44% examples, 870721 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:20: EPOCH 3 - PROGRESS: at 58.62% examples, 874309 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:21: EPOCH 3 - PROGRESS: at 60.75% examples, 879637 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:22: EPOCH 3 - PROGRESS: at 63.00% examples, 884400 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:23: EPOCH 3 - PROGRESS: at 64.92% examples, 885953 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:24: EPOCH 3 - PROGRESS: at 66.93% examples, 887282 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:25: EPOCH 3 - PROGRESS: at 68.75% examples, 889040 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:26: EPOCH 3 - PROGRESS: at 70.82% examples, 889084 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:27: EPOCH 3 - PROGRESS: at 72.82% examples, 892040 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:28: EPOCH 3 - PROGRESS: at 74.88% examples, 894263 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:29: EPOCH 3 - PROGRESS: at 77.05% examples, 894294 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 16:41:30: EPOCH 3 - PROGRESS: at 78.92% examples, 890308 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:31: EPOCH 3 - PROGRESS: at 80.50% examples, 885080 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:32: EPOCH 3 - PROGRESS: at 82.45% examples, 885745 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:33: EPOCH 3 - PROGRESS: at 84.13% examples, 882069 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:34: EPOCH 3 - PROGRESS: at 85.86% examples, 883199 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:35: EPOCH 3 - PROGRESS: at 87.66% examples, 884288 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:36: EPOCH 3 - PROGRESS: at 89.76% examples, 883924 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:37: EPOCH 3 - PROGRESS: at 91.84% examples, 885829 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:38: EPOCH 3 - PROGRESS: at 94.31% examples, 886752 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:39: EPOCH 3 - PROGRESS: at 96.13% examples, 887500 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:40: EPOCH 3 - PROGRESS: at 98.06% examples, 885842 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:41: EPOCH 3 - PROGRESS: at 99.74% examples, 884688 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:41: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:41:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:41:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:41:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:41:41: EPOCH - 3 : training on 63696185 raw words (45786073 effective words) took 51.8s, 884299 effective words/s\n",
      "INFO - 16:41:42: EPOCH 4 - PROGRESS: at 2.00% examples, 883690 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:43: EPOCH 4 - PROGRESS: at 4.23% examples, 930115 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:44: EPOCH 4 - PROGRESS: at 6.31% examples, 937503 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:45: EPOCH 4 - PROGRESS: at 8.08% examples, 919643 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:46: EPOCH 4 - PROGRESS: at 10.38% examples, 925490 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:47: EPOCH 4 - PROGRESS: at 12.53% examples, 934633 words/s, in_qsize 7, out_qsize 1\n",
      "INFO - 16:41:48: EPOCH 4 - PROGRESS: at 14.84% examples, 947638 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:49: EPOCH 4 - PROGRESS: at 16.96% examples, 953330 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:50: EPOCH 4 - PROGRESS: at 19.24% examples, 955556 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:51: EPOCH 4 - PROGRESS: at 21.54% examples, 962070 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:52: EPOCH 4 - PROGRESS: at 23.76% examples, 964427 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:53: EPOCH 4 - PROGRESS: at 25.75% examples, 959980 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:54: EPOCH 4 - PROGRESS: at 27.74% examples, 955677 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:55: EPOCH 4 - PROGRESS: at 29.70% examples, 957991 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:56: EPOCH 4 - PROGRESS: at 32.03% examples, 963353 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:57: EPOCH 4 - PROGRESS: at 34.29% examples, 962983 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:58: EPOCH 4 - PROGRESS: at 36.54% examples, 963860 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:41:59: EPOCH 4 - PROGRESS: at 38.32% examples, 962374 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:00: EPOCH 4 - PROGRESS: at 40.42% examples, 965532 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:01: EPOCH 4 - PROGRESS: at 42.57% examples, 966820 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:02: EPOCH 4 - PROGRESS: at 44.90% examples, 969143 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:03: EPOCH 4 - PROGRESS: at 47.18% examples, 967158 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:04: EPOCH 4 - PROGRESS: at 49.19% examples, 964677 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:05: EPOCH 4 - PROGRESS: at 51.36% examples, 967200 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:06: EPOCH 4 - PROGRESS: at 53.71% examples, 970619 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:07: EPOCH 4 - PROGRESS: at 55.91% examples, 973787 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:08: EPOCH 4 - PROGRESS: at 58.06% examples, 973781 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:09: EPOCH 4 - PROGRESS: at 60.27% examples, 975811 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:10: EPOCH 4 - PROGRESS: at 62.47% examples, 976353 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 16:42:11: EPOCH 4 - PROGRESS: at 64.63% examples, 978887 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:12: EPOCH 4 - PROGRESS: at 66.90% examples, 981152 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:13: EPOCH 4 - PROGRESS: at 68.83% examples, 983262 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:14: EPOCH 4 - PROGRESS: at 71.18% examples, 984939 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:15: EPOCH 4 - PROGRESS: at 73.32% examples, 985769 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:16: EPOCH 4 - PROGRESS: at 75.58% examples, 987153 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:17: EPOCH 4 - PROGRESS: at 78.08% examples, 988796 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:18: EPOCH 4 - PROGRESS: at 80.67% examples, 990366 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:19: EPOCH 4 - PROGRESS: at 82.94% examples, 992030 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:20: EPOCH 4 - PROGRESS: at 85.19% examples, 993277 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:21: EPOCH 4 - PROGRESS: at 87.09% examples, 994814 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:22: EPOCH 4 - PROGRESS: at 89.55% examples, 996501 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:23: EPOCH 4 - PROGRESS: at 91.81% examples, 997708 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:24: EPOCH 4 - PROGRESS: at 94.40% examples, 997645 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:25: EPOCH 4 - PROGRESS: at 96.20% examples, 995317 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:26: EPOCH 4 - PROGRESS: at 98.29% examples, 992658 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:42:27: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:42:27: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:42:27: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:42:27: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:42:27: EPOCH - 4 : training on 63696185 raw words (45785587 effective words) took 46.2s, 991369 effective words/s\n",
      "INFO - 16:42:28: EPOCH 5 - PROGRESS: at 1.99% examples, 873972 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:29: EPOCH 5 - PROGRESS: at 3.97% examples, 878252 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:30: EPOCH 5 - PROGRESS: at 6.29% examples, 926430 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:31: EPOCH 5 - PROGRESS: at 8.31% examples, 949677 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:32: EPOCH 5 - PROGRESS: at 10.82% examples, 961521 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:33: EPOCH 5 - PROGRESS: at 13.05% examples, 970227 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:34: EPOCH 5 - PROGRESS: at 15.23% examples, 976738 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:35: EPOCH 5 - PROGRESS: at 17.45% examples, 975650 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:36: EPOCH 5 - PROGRESS: at 19.69% examples, 973959 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:37: EPOCH 5 - PROGRESS: at 21.92% examples, 976645 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:38: EPOCH 5 - PROGRESS: at 23.92% examples, 970555 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:39: EPOCH 5 - PROGRESS: at 25.70% examples, 956590 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:40: EPOCH 5 - PROGRESS: at 27.05% examples, 936248 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:41: EPOCH 5 - PROGRESS: at 29.20% examples, 935910 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:42: EPOCH 5 - PROGRESS: at 31.25% examples, 937238 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:43: EPOCH 5 - PROGRESS: at 33.79% examples, 943947 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:44: EPOCH 5 - PROGRESS: at 35.67% examples, 937269 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:45: EPOCH 5 - PROGRESS: at 36.89% examples, 920473 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:46: EPOCH 5 - PROGRESS: at 38.74% examples, 922276 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:47: EPOCH 5 - PROGRESS: at 40.85% examples, 925279 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:48: EPOCH 5 - PROGRESS: at 43.09% examples, 929665 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:49: EPOCH 5 - PROGRESS: at 45.30% examples, 929521 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:50: EPOCH 5 - PROGRESS: at 47.80% examples, 933645 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:51: EPOCH 5 - PROGRESS: at 49.85% examples, 936403 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:52: EPOCH 5 - PROGRESS: at 51.68% examples, 932949 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:53: EPOCH 5 - PROGRESS: at 53.15% examples, 918396 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:54: EPOCH 5 - PROGRESS: at 55.00% examples, 920537 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:55: EPOCH 5 - PROGRESS: at 57.19% examples, 922768 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:56: EPOCH 5 - PROGRESS: at 59.32% examples, 924710 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:57: EPOCH 5 - PROGRESS: at 61.45% examples, 927958 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:58: EPOCH 5 - PROGRESS: at 63.62% examples, 930982 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:42:59: EPOCH 5 - PROGRESS: at 65.80% examples, 933554 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:00: EPOCH 5 - PROGRESS: at 68.09% examples, 937088 words/s, in_qsize 8, out_qsize 0\n",
      "INFO - 16:43:01: EPOCH 5 - PROGRESS: at 69.85% examples, 937202 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:02: EPOCH 5 - PROGRESS: at 71.67% examples, 935889 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:03: EPOCH 5 - PROGRESS: at 73.85% examples, 936558 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:04: EPOCH 5 - PROGRESS: at 75.95% examples, 937022 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 16:43:05: EPOCH 5 - PROGRESS: at 78.41% examples, 939460 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:06: EPOCH 5 - PROGRESS: at 80.83% examples, 942027 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:07: EPOCH 5 - PROGRESS: at 82.98% examples, 942250 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:08: EPOCH 5 - PROGRESS: at 85.13% examples, 942740 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:09: EPOCH 5 - PROGRESS: at 86.88% examples, 943261 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:10: EPOCH 5 - PROGRESS: at 88.98% examples, 944412 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:11: EPOCH 5 - PROGRESS: at 90.99% examples, 943696 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:12: EPOCH 5 - PROGRESS: at 93.58% examples, 943721 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:13: EPOCH 5 - PROGRESS: at 95.55% examples, 944205 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:14: EPOCH 5 - PROGRESS: at 97.61% examples, 943689 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:15: EPOCH 5 - PROGRESS: at 99.57% examples, 943244 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 16:43:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 16:43:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:43:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:43:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:43:16: EPOCH - 5 : training on 63696185 raw words (45781662 effective words) took 48.5s, 943479 effective words/s\n",
      "INFO - 16:43:16: training on a 318480925 raw words (228921697 effective words) took 245.9s, 930878 effective words/s\n",
      "INFO - 16:43:16: saving Word2Vec object under 300features_40minwords_10context.wv.model, separately None\n",
      "INFO - 16:43:16: not storing attribute vectors_norm\n",
      "INFO - 16:43:16: not storing attribute cum_table\n",
      "INFO - 16:43:17: saved 300features_40minwords_10context.wv.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x131e01668>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_hat(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:45:37: loading Word2Vec object from 300features_40minwords_10context.wv.model\n",
      "INFO - 16:45:38: loading wv recursively from 300features_40minwords_10context.wv.model.wv.* with mmap=None\n",
      "INFO - 16:45:38: setting ignored attribute vectors_norm to None\n",
      "INFO - 16:45:38: loading vocabulary recursively from 300features_40minwords_10context.wv.model.vocabulary.* with mmap=None\n",
      "INFO - 16:45:38: loading trainables recursively from 300features_40minwords_10context.wv.model.trainables.* with mmap=None\n",
      "INFO - 16:45:38: setting ignored attribute cum_table to None\n",
      "INFO - 16:45:38: loaded 300features_40minwords_10context.wv.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"300features_40minwords_10context.wv.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review most similar words\n",
    "Get initial evaluation of the word vectors by analyzing the most similar words for a few interesting words in the text. \n",
    "\n",
    "Choose words yourself, and find the most similar words to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:52:02: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hound', 0.6047170758247375),\n",
       " ('puppy', 0.5669090747833252),\n",
       " ('cat', 0.5553213357925415),\n",
       " ('dogs', 0.5495033860206604),\n",
       " ('barking', 0.5416650772094727),\n",
       " ('hog', 0.5347200036048889),\n",
       " ('frog', 0.5303950905799866),\n",
       " ('barkin', 0.5250210165977478),\n",
       " ('bark', 0.5211504697799683),\n",
       " ('doggy', 0.48853635787963867)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"dog\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baby', 0.554186224937439),\n",
       " ('loving', 0.5354353189468384),\n",
       " ('heart', 0.5152585506439209),\n",
       " ('oh', 0.5066409111022949),\n",
       " ('you', 0.4939931631088257),\n",
       " ('true', 0.48410582542419434),\n",
       " ('darling', 0.4810052216053009),\n",
       " ('lovin', 0.4613601565361023),\n",
       " ('me', 0.4605832099914551),\n",
       " ('darlin', 0.4548726975917816)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"love\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('battle', 0.6541935801506042),\n",
       " ('waging', 0.6304421424865723),\n",
       " ('wars', 0.6265519261360168),\n",
       " ('waged', 0.5722817778587341),\n",
       " ('civil', 0.5561788082122803),\n",
       " ('battles', 0.496185839176178),\n",
       " ('tug', 0.48518654704093933),\n",
       " ('battlefield', 0.4778044819831848),\n",
       " ('fighting', 0.477022647857666),\n",
       " ('wage', 0.4723787009716034)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"war\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bluest', 0.5335527658462524),\n",
       " ('gray', 0.5190421938896179),\n",
       " ('grey', 0.47112494707107544),\n",
       " ('starry', 0.43493953347206116),\n",
       " ('cloudy', 0.4216483235359192),\n",
       " ('starlit', 0.4208819270133972),\n",
       " ('hue', 0.4200236201286316),\n",
       " ('tangerine', 0.4178546667098999),\n",
       " ('colour', 0.4062366485595703),\n",
       " ('misty', 0.40504148602485657)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"blue\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('france', 0.6699743866920471),\n",
       " ('spain', 0.5838769674301147),\n",
       " ('hilton', 0.5828134417533875),\n",
       " ('tokyo', 0.5712754726409912),\n",
       " ('italy', 0.5608839988708496),\n",
       " ('venice', 0.5596826672554016),\n",
       " ('london', 0.5530416369438171),\n",
       " ('miami', 0.5352116823196411),\n",
       " ('brazil', 0.5246003866195679),\n",
       " ('rome', 0.5206924676895142)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"paris\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jewish', 0.5946222543716431),\n",
       " ('buddhist', 0.5576699376106262),\n",
       " ('muslim', 0.5428418517112732),\n",
       " ('caucasian', 0.5240269303321838),\n",
       " ('bishop', 0.5107120275497437),\n",
       " ('goat', 0.5064100027084351),\n",
       " ('christian', 0.4977492392063141),\n",
       " ('haitian', 0.49670469760894775),\n",
       " ('reggie', 0.4932796061038971),\n",
       " ('italian', 0.486927330493927)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"jew\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors Algebra\n",
    "We've seen in class examples of algebraic games on the word vectors (e.g. man - woman + king = queen ). \n",
    "\n",
    "Try a few vector algebra terms, and evaluate how well they work. Try to use the Cosine distance and compare it to the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.5984432697296143),\n",
       " ('kings', 0.4317205548286438),\n",
       " ('empress', 0.3948866128921509),\n",
       " ('princess', 0.39446741342544556),\n",
       " ('homecoming', 0.3881065249443054),\n",
       " ('throne', 0.3874613642692566),\n",
       " ('crowning', 0.3848448097705841),\n",
       " ('crowned', 0.38424116373062134),\n",
       " ('crown', 0.3822874128818512),\n",
       " ('majesty', 0.3679766058921814)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8952284455299377),\n",
       " ('empress', 0.7569559812545776),\n",
       " ('princess', 0.7516407370567322),\n",
       " ('homecoming', 0.7510775923728943),\n",
       " ('kings', 0.7475972771644592),\n",
       " ('sweetest', 0.744684100151062),\n",
       " ('throne', 0.7338636517524719),\n",
       " ('crown', 0.7334924936294556),\n",
       " ('crowning', 0.7326667904853821),\n",
       " ('newborn', 0.7302643656730652)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.7405559420585632),\n",
       " ('queen', 0.6157742738723755),\n",
       " ('woman', 0.43306297063827515),\n",
       " ('kings', 0.43230652809143066),\n",
       " ('princess', 0.4136694669723511),\n",
       " ('throne', 0.40156853199005127),\n",
       " ('empress', 0.3987256586551666),\n",
       " ('homecoming', 0.394988477230072),\n",
       " ('crowning', 0.39387229084968567),\n",
       " ('crown', 0.39278435707092285)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = w2v_model.wv['king'] - w2v_model.wv['man'] + w2v_model.wv['woman']\n",
    "w2v_model.wv.similar_by_vector(vector, topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance =  0.55022025\n",
      "Euclidean distance =  30.819527\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "cosine_distance = w2v_model.wv.similarity(\"woman\", \"girl\")\n",
    "print(\"Cosine distance = \", cosine_distance)\n",
    "\n",
    "import numpy as np\n",
    "euclidean_distance = np.linalg.norm(w2v_model.wv[\"women\"] - w2v_model.wv[\"girl\"])\n",
    "print(\"Euclidean distance = \", euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Estimate sentiment of words using word vectors.  \n",
    "In this section, we'll use the SemEval-2015 English Twitter Sentiment Lexicon.  \n",
    "The lexicon was used as an official test set in the SemEval-2015 shared Task #10: Subtask E, and contains a polarity score for words in range -1 (negative) to 1 (positive) - http://saifmohammad.com/WebPages/SCL.html#OPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a classifier for the sentiment of a word given its word vector. Split the data to a train and test sets, and report the model performance on both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your trained model from the previous question to predict the sentiment score of words in the lyrics corpus that are not part of the original sentiment dataset. Review the words with the highest positive and negative sentiment. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Word Vectors\n",
    "In this section, you'll plot words on a 2D grid based on their inner similarity. We'll use the tSNE transformation to reduce dimensions from 300 to 2. You can get sample code from https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial or other tutorials online.\n",
    "\n",
    "Perform the following:\n",
    "- Keep only the 3,000 most frequent words (after removing stopwords)\n",
    "- For this list, compute for each word its relative abundance in each of the genres\n",
    "- Compute the ratio between the proportion of each word in each genre and the proportion of the word in the entire corpus (the background distribution)\n",
    "- Pick the top 50 words for each genre. These words give good indication for that genre. Join the words from all genres into a single list of top significant words. \n",
    "- Compute tSNE transformation to 2D for all words, based on their word vectors\n",
    "- Plot the list of the top significant words in 2D. Next to each word output its text. The color of each point should indicate the genre for which it is most significant.\n",
    "\n",
    "You might prefer to use a different number of points or a slightly different methodology for improved results.  \n",
    "Analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-4f8e57bf9888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtsne_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_subset' is not defined"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(data_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "In this section, you'll build a text classifier, determining the genre of a song based on its lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using Bag-of-Words\n",
    "Build a Naive Bayes classifier based on the bag of Words.  \n",
    "You will need to divide your dataset into a train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the classification report - precision, recall, f1 for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using Word Vectors\n",
    "#### Average word vectors\n",
    "Do the same, using a classifier that averages the word vectors of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfIdf Weighting\n",
    "Do the same, using a classifier that averages the word vectors of words in the document, weighting each word by its TfIdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using ConvNet\n",
    "Do the same, using a ConvNet.  \n",
    "The ConvNet should get as input a 2D matrix where each column is an embedding vector of a single word, and words are in order. Use zero padding so that all matrices have a similar length.  \n",
    "Some songs might be very long. Trim them so you keep a maximum of 128 words (after cleaning stop words and rare words).  \n",
    "Initialize the embedding layer using the word vectors that you've trained before, but allow them to change during training.  \n",
    "\n",
    "Extra: Try training the ConvNet with 2 slight modifications:\n",
    "1. freezing the the weights trained using Word2vec (preventing it from updating)\n",
    "1. random initialization of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are encouraged to try this question on your own.  \n",
    "\n",
    "You might prefer to get ideas from the paper \"Convolutional Neural Networks for Sentence Classification\" (Kim 2014, [link](https://arxiv.org/abs/1408.5882)).\n",
    "\n",
    "There are several implementations of the paper code in PyTorch online (see for example [this repo](https://github.com/prakashpandey9/Text-Classification-Pytorch) for a PyTorch implementation of CNN and other architectures for text classification). If you get stuck, they might provide you with a reference for your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
