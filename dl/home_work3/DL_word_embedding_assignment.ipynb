{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GreJerdev/Y-data/blob/master/dl/home_work3/DL_word_embedding_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwOajtwGzpEQ"
   },
   "source": [
    "# Word Embedding - Home Assigment\n",
    "## Dr. Omri Allouche 2018. YData Deep Learning Course\n",
    "\n",
    "[Open in Google Colab](https://colab.research.google.com/github/omriallouche/deep_learning_course/blob/master/DL_word_embedding_assignment.ipynb)\n",
    "    \n",
    "    \n",
    "In this exercise, you'll use word vectors trained on a corpus of 380,000 lyrics of songs from MetroLyrics (https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics).  \n",
    "The dataset contains these fields for each song, in CSV format:\n",
    "1. index\n",
    "1. song\n",
    "1. year\n",
    "1. artist\n",
    "1. genre\n",
    "1. lyrics\n",
    "\n",
    "Before doing this exercise, we recommend that you go over the \"Bag of words meets bag of popcorn\" tutorial (https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
    "\n",
    "Other recommended resources:\n",
    "- https://rare-technologies.com/word2vec-tutorial/\n",
    "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGblAtuKzpER"
   },
   "source": [
    "### Train word vectors\n",
    "Train word vectors using the Skipgram Word2vec algorithm and the gensim package.\n",
    "Make sure you perform the following:\n",
    "- Tokenize words\n",
    "- Lowercase all words\n",
    "- Remove punctuation marks\n",
    "- Remove rare words\n",
    "- Remove stopwords\n",
    "\n",
    "Use 300 as the dimension of the word vectors. Try different context sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTwHlaTEzpES"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2173422 labeled data reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ego-remix</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh baby, how you doing?\\nYou know I'm gonna cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>then-tell-me</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>playin' everything so easy,\\nit's like you see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>honesty</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>you-are-my-rock</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>black-culture</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Party the people, the people the party it's po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index             song  year           artist genre  \\\n",
       "0      0        ego-remix  2009  beyonce-knowles   Pop   \n",
       "1      1     then-tell-me  2009  beyonce-knowles   Pop   \n",
       "2      2          honesty  2009  beyonce-knowles   Pop   \n",
       "3      3  you-are-my-rock  2009  beyonce-knowles   Pop   \n",
       "4      4    black-culture  2009  beyonce-knowles   Pop   \n",
       "\n",
       "                                              lyrics  \n",
       "0  Oh baby, how you doing?\\nYou know I'm gonna cu...  \n",
       "1  playin' everything so easy,\\nit's like you see...  \n",
       "2  If you search\\nFor tenderness\\nIt isn't hard t...  \n",
       "3  Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...  \n",
       "4  Party the people, the people the party it's po...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from files \n",
    "data = pd.read_csv( \"lyrics.csv\", header=0, delimiter=\",\" )\n",
    "\n",
    "# Verify the number of row that were read (2,173,422 in total)\n",
    "print (\"Read %d labeled data reviews\" % (data.size))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362237"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"lyrics\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "\n",
    "def lyrics_to_wordlist( lyrics, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    lyrics_text = BeautifulSoup(lyrics).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    lyrics_text = re.sub(\"[^a-zA-Z]\",\" \", lyrics_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = lyrics_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def lyrics_to_sentences( lyrics, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    if(len(lyrics)== 0):\n",
    "        return []\n",
    "    raw_sentences = tokenizer.tokenize(lyrics.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( lyrics_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g/.local/lib/python3.6/site-packages/bs4/__init__.py:314: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/362237\n",
      "2000/362237\n",
      "3000/362237\n",
      "4000/362237\n",
      "5000/362237\n",
      "7000/362237\n",
      "9000/362237\n",
      "12000/362237\n",
      "13000/362237\n",
      "14000/362237\n",
      "16000/362237\n",
      "17000/362237\n",
      "18000/362237\n",
      "19000/362237\n",
      "22000/362237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g/.local/lib/python3.6/site-packages/bs4/__init__.py:314: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/362237\n",
      "26000/362237\n",
      "27000/362237\n",
      "29000/362237\n",
      "33000/362237\n",
      "34000/362237\n",
      "36000/362237\n",
      "37000/362237\n",
      "38000/362237\n",
      "40000/362237\n",
      "42000/362237\n",
      "43000/362237\n",
      "44000/362237\n",
      "45000/362237\n",
      "46000/362237\n",
      "47000/362237\n",
      "48000/362237\n",
      "50000/362237\n",
      "53000/362237\n",
      "54000/362237\n",
      "56000/362237\n",
      "57000/362237\n",
      "58000/362237\n",
      "61000/362237\n",
      "62000/362237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g/.local/lib/python3.6/site-packages/bs4/__init__.py:314: UserWarning: \"b'/'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000/362237\n",
      "68000/362237\n",
      "69000/362237\n",
      "72000/362237\n",
      "73000/362237\n",
      "75000/362237\n",
      "76000/362237\n",
      "77000/362237\n",
      "78000/362237\n",
      "79000/362237\n",
      "80000/362237\n",
      "81000/362237\n",
      "83000/362237\n",
      "84000/362237\n",
      "85000/362237\n",
      "86000/362237\n",
      "87000/362237\n",
      "89000/362237\n",
      "91000/362237\n",
      "92000/362237\n",
      "93000/362237\n",
      "94000/362237\n",
      "95000/362237\n",
      "96000/362237\n",
      "97000/362237\n",
      "98000/362237\n",
      "100000/362237\n",
      "102000/362237\n",
      "103000/362237\n",
      "105000/362237\n",
      "107000/362237\n",
      "108000/362237\n",
      "109000/362237\n",
      "110000/362237\n",
      "112000/362237\n",
      "113000/362237\n",
      "115000/362237\n",
      "116000/362237\n",
      "117000/362237\n",
      "118000/362237\n",
      "119000/362237\n",
      "121000/362237\n",
      "122000/362237\n",
      "124000/362237\n",
      "125000/362237\n",
      "126000/362237\n",
      "127000/362237\n",
      "128000/362237\n",
      "129000/362237\n",
      "130000/362237\n",
      "131000/362237\n",
      "132000/362237\n",
      "134000/362237\n",
      "135000/362237\n",
      "136000/362237\n",
      "137000/362237\n",
      "138000/362237\n",
      "139000/362237\n",
      "140000/362237\n",
      "141000/362237\n",
      "143000/362237\n",
      "144000/362237\n",
      "145000/362237\n",
      "146000/362237\n",
      "147000/362237\n",
      "148000/362237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g/.local/lib/python3.6/site-packages/bs4/__init__.py:314: UserWarning: \"b'//'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149000/362237\n",
      "151000/362237\n",
      "152000/362237\n",
      "154000/362237\n",
      "157000/362237\n",
      "158000/362237\n",
      "160000/362237\n",
      "161000/362237\n",
      "162000/362237\n",
      "164000/362237\n",
      "165000/362237\n",
      "166000/362237\n",
      "170000/362237\n",
      "171000/362237\n",
      "172000/362237\n",
      "174000/362237\n",
      "175000/362237\n",
      "177000/362237\n",
      "178000/362237\n",
      "179000/362237\n",
      "180000/362237\n",
      "181000/362237\n",
      "182000/362237\n",
      "183000/362237\n",
      "188000/362237\n",
      "189000/362237\n",
      "190000/362237\n",
      "191000/362237\n",
      "192000/362237\n",
      "193000/362237\n",
      "195000/362237\n",
      "196000/362237\n",
      "197000/362237\n",
      "199000/362237\n",
      "200000/362237\n",
      "201000/362237\n",
      "202000/362237\n",
      "205000/362237\n",
      "206000/362237\n",
      "207000/362237\n",
      "208000/362237\n",
      "209000/362237\n",
      "211000/362237\n",
      "212000/362237\n",
      "215000/362237\n",
      "218000/362237\n",
      "219000/362237\n",
      "220000/362237\n",
      "221000/362237\n",
      "222000/362237\n",
      "223000/362237\n",
      "225000/362237\n",
      "227000/362237\n",
      "228000/362237\n",
      "229000/362237\n",
      "230000/362237\n",
      "231000/362237\n",
      "232000/362237\n",
      "233000/362237\n",
      "234000/362237\n",
      "235000/362237\n",
      "237000/362237\n",
      "240000/362237\n",
      "241000/362237\n",
      "243000/362237\n",
      "244000/362237\n",
      "245000/362237\n",
      "247000/362237\n",
      "248000/362237\n",
      "250000/362237\n",
      "251000/362237\n",
      "252000/362237\n",
      "253000/362237\n",
      "254000/362237\n",
      "255000/362237\n",
      "256000/362237\n",
      "257000/362237\n",
      "258000/362237\n",
      "259000/362237\n",
      "260000/362237\n",
      "261000/362237\n",
      "263000/362237\n",
      "265000/362237\n",
      "266000/362237\n",
      "267000/362237\n",
      "269000/362237\n",
      "270000/362237\n",
      "271000/362237\n",
      "272000/362237\n",
      "273000/362237\n",
      "274000/362237\n",
      "275000/362237\n",
      "276000/362237\n",
      "277000/362237\n",
      "280000/362237\n",
      "281000/362237\n",
      "282000/362237\n",
      "283000/362237\n",
      "284000/362237\n",
      "285000/362237\n",
      "286000/362237\n",
      "287000/362237\n",
      "288000/362237\n",
      "290000/362237\n",
      "292000/362237\n",
      "294000/362237\n",
      "295000/362237\n",
      "296000/362237\n",
      "297000/362237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g/.local/lib/python3.6/site-packages/bs4/__init__.py:314: UserWarning: \"b'/.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298000/362237\n",
      "300000/362237\n",
      "301000/362237\n",
      "302000/362237\n",
      "303000/362237\n",
      "304000/362237\n",
      "305000/362237\n",
      "306000/362237\n",
      "307000/362237\n",
      "308000/362237\n",
      "309000/362237\n",
      "310000/362237\n",
      "311000/362237\n",
      "312000/362237\n",
      "315000/362237\n",
      "316000/362237\n",
      "319000/362237\n",
      "320000/362237\n",
      "321000/362237\n",
      "323000/362237\n",
      "325000/362237\n",
      "329000/362237\n",
      "330000/362237\n",
      "331000/362237\n",
      "332000/362237\n",
      "333000/362237\n",
      "336000/362237\n",
      "339000/362237\n",
      "340000/362237\n",
      "341000/362237\n",
      "342000/362237\n",
      "343000/362237\n",
      "345000/362237\n",
      "346000/362237\n",
      "348000/362237\n",
      "349000/362237\n",
      "350000/362237\n",
      "352000/362237\n",
      "353000/362237\n",
      "354000/362237\n",
      "355000/362237\n",
      "356000/362237\n",
      "357000/362237\n",
      "358000/362237\n",
      "360000/362237\n",
      "361000/362237\n",
      "362000/362237\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "print (\"Parsing sentences from training set\")\n",
    "counter = 0\n",
    "total_number_of_rows = len(data[\"lyrics\"])\n",
    "for lyrics in data[\"lyrics\"]:\n",
    "    counter +=1\n",
    "    if(lyrics == lyrics):  \n",
    "        if counter % 1000 == 0:\n",
    "            print(f'{counter}/{total_number_of_rows}')\n",
    "        sentences += lyrics_to_sentences(lyrics, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Step 2\n",
    "with open('sentences.b', 'wb') as config_dictionary_file:\n",
    "  # Step 3\n",
    "  pickle.dump(sentences, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 3000    # Word vector dimensionality                      \n",
    "min_word_count = 30   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ts2zHcAXzpEV"
   },
   "source": [
    "### Review most similar words\n",
    "Get initial evaluation of the word vectors by analyzing the most similar words for a few interesting words in the text. \n",
    "\n",
    "Choose words yourself, and find the most similar words to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRbyxePnzpEV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nashville', 0.4711722135543823),\n",
       " ('texas', 0.42542052268981934),\n",
       " ('hillbilly', 0.425087571144104),\n",
       " ('farm', 0.42091020941734314),\n",
       " ('southern', 0.41700053215026855),\n",
       " ('tennessee', 0.4128624200820923),\n",
       " ('homegrown', 0.40444788336753845),\n",
       " ('western', 0.4026687443256378),\n",
       " ('illinois', 0.39904969930648804),\n",
       " ('carolina', 0.3977320194244385)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alone', 0.44149452447891235),\n",
       " ('back', 0.39174431562423706),\n",
       " ('where', 0.36443257331848145),\n",
       " ('homesick', 0.3211636543273926),\n",
       " ('payphone', 0.3109445571899414),\n",
       " ('roam', 0.3012731075286865),\n",
       " ('safely', 0.29832303524017334),\n",
       " ('phone', 0.2923828363418579),\n",
       " ('lonely', 0.28657495975494385),\n",
       " ('holidays', 0.2828644812107086)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.529973030090332),\n",
       " ('queen', 0.4928017258644104),\n",
       " ('crowned', 0.4898328483104706),\n",
       " ('ruler', 0.46860259771347046),\n",
       " ('rodney', 0.4455219805240631),\n",
       " ('luther', 0.43770158290863037),\n",
       " ('crowning', 0.42637744545936584),\n",
       " ('hong', 0.4263729453086853),\n",
       " ('throne', 0.4238130450248718),\n",
       " ('simba', 0.411740243434906)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3KgAHY_zpEX"
   },
   "source": [
    "### Word Vectors Algebra\n",
    "We've seen in class examples of algebraic games on the word vectors (e.g. man - woman + king = queen ). \n",
    "\n",
    "Try a few vector algebra terms, and evaluate how well they work. Try to use the Cosine distance and compare it to the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqsdcAefzpEY",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04580969 -0.07200838 -0.00492593 ... -0.03154615  0.01647244\n",
      " -0.00359621]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('king', 0.6875251531600952),\n",
       " ('queen', 0.46576985716819763),\n",
       " ('woman', 0.42273080348968506),\n",
       " ('kings', 0.3725214898586273),\n",
       " ('crowned', 0.3481811285018921),\n",
       " ('princess', 0.3264182209968567),\n",
       " ('crown', 0.320339560508728),\n",
       " ('goddess', 0.313880980014801),\n",
       " ('palace', 0.30315709114074707),\n",
       " ('hong', 0.2977418303489685)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.wv['king'] - model.wv['man'] + model.wv['woman']\n",
    "print(vector)\n",
    "model.wv.similar_by_vector(vector, topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance =  0.030345669\n",
      "Euclidean distance =  1.3925906\n"
     ]
    }
   ],
   "source": [
    "cosine_distance = model.wv.similarity(\"car\", \"plan\")\n",
    "print(\"Cosine distance = \", cosine_distance)\n",
    "\n",
    "euclidean_distance = np.linalg.norm(model.wv[\"car\"] - model.wv[\"plan\"])\n",
    "print(\"Euclidean distance = \", euclidean_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w77_5NJRzpEb"
   },
   "source": [
    "## Sentiment Analysis\n",
    "Estimate sentiment of words using word vectors.  \n",
    "In this section, we'll use the SemEval-2015 English Twitter Sentiment Lexicon.  \n",
    "The lexicon was used as an official test set in the SemEval-2015 shared Task #10: Subtask E, and contains a polarity score for words in range -1 (negative) to 1 (positive) - http://saifmohammad.com/WebPages/SCL.html#OPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgsegRh6zpEc"
   },
   "source": [
    "Build a classifier for the sentiment of a word given its word vector. Split the data to a train and test sets, and report the model performance on both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMvIKiB9zpEc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4712 labeled data reviews\n"
     ]
    }
   ],
   "source": [
    "scl_opp = pd.read_csv( \"SCL-OPP.txt\",header=None, delimiter=\"\\t\")\n",
    "\n",
    "# Verify the number of row that were read (2,173,422 in total)\n",
    "print (\"Read %d labeled data reviews\" % (scl_opp.size))\n",
    "scl_opp.head()\n",
    "\n",
    "tokens_dic = {}\n",
    "\n",
    "for i in range(scl_opp.shape[0]):\n",
    "    tokens_dic[scl_opp.iloc[i][0]] = scl_opp.iloc[i][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazing</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>0.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smiling</td>\n",
       "      <td>0.953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>0.953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happiness</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  sentiment\n",
       "0    amazing      1.000\n",
       "1       love      0.969\n",
       "2    smiling      0.953\n",
       "3  wonderful      0.953\n",
       "4  happiness      0.938"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All the words in our lyrics corpus\n",
    "vocab =  list(model.wv.vocab.keys())\n",
    "#All the words shared between the sentiment lexicon and our lyrics corpus.\n",
    "lexicon_words = []\n",
    "for key,value in tokens_dic.items():\n",
    "    if key in vocab:\n",
    "        lexicon_words.append([key,value])\n",
    "lexicon = pd.DataFrame(lexicon_words, columns=[\"word\",\"sentiment\"])\n",
    "lexicon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwTiyFW8zpEf"
   },
   "source": [
    "Use your trained model from the previous question to predict the sentiment score of words in the lyrics corpus that are not part of the original sentiment dataset. Review the words with the highest positive and negative sentiment. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 3.548735396530034e-12, Test Score 0.33717765845892406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "w2vector = model[lexicon.word]\n",
    "X_train, X_test, y_train, y_test = train_test_split(w2vector,lexicon.sentiment, test_size = 0.2, random_state=42)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "print(\"Train Score: {}, Test Score {}\".format(mse(lr.predict(X_train),y_train),mse(lr.predict(X_test),y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n",
      "/home/g/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#the next part will be to use our lr model to predict sentiment score for any word embeddings\n",
    "not_in_lexicon = []\n",
    "for word in vocab:\n",
    "    if word not in lexicon['word'].values:\n",
    "        not_in_lexicon.append(word)\n",
    "\n",
    "values = lr.predict(model[not_in_lexicon])\n",
    "\n",
    "sents_df = pd.DataFrame((not_in_lexicon, values)).T\n",
    "sents_df.columns = [\"word\",\"sentiment\"]\n",
    "sents_df.Sentiment = sents_df.sentiment / max(np.abs(sents_df.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16338</th>\n",
       "      <td>dismayed</td>\n",
       "      <td>-2.15712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>stalking</td>\n",
       "      <td>-2.09456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326</th>\n",
       "      <td>bastard</td>\n",
       "      <td>-2.05636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22901</th>\n",
       "      <td>brung</td>\n",
       "      <td>-2.0045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4129</th>\n",
       "      <td>snitch</td>\n",
       "      <td>-1.98548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30275</th>\n",
       "      <td>ohoho</td>\n",
       "      <td>2.05039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30367</th>\n",
       "      <td>schlau</td>\n",
       "      <td>2.05304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>enjoy</td>\n",
       "      <td>2.05487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>trim</td>\n",
       "      <td>2.19221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33410</th>\n",
       "      <td>rikki</td>\n",
       "      <td>2.29582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35013 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word sentiment\n",
       "16338  dismayed  -2.15712\n",
       "2998   stalking  -2.09456\n",
       "4326    bastard  -2.05636\n",
       "22901     brung   -2.0045\n",
       "4129     snitch  -1.98548\n",
       "...         ...       ...\n",
       "30275     ohoho   2.05039\n",
       "30367    schlau   2.05304\n",
       "629       enjoy   2.05487\n",
       "5018       trim   2.19221\n",
       "33410     rikki   2.29582\n",
       "\n",
       "[35013 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_df.sort_values(by=\"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHl_oWv3zpEh"
   },
   "source": [
    "### Visualize Word Vectors\n",
    "In this section, you'll plot words on a 2D grid based on their inner similarity. We'll use the tSNE transformation to reduce dimensions from 300 to 2. You can get sample code from https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial or other tutorials online.\n",
    "\n",
    "Perform the following:\n",
    "- Keep only the 3,000 most frequent words (after removing stopwords)\n",
    "- For this list, compute for each word its relative abundance in each of the genres\n",
    "- Compute the ratio between the proportion of each word in each genre and the proportion of the word in the entire corpus (the background distribution)\n",
    "- Pick the top 50 words for each genre. These words give good indication for that genre. Join the words from all genres into a single list of top significant words. \n",
    "- Compute tSNE transformation to 2D for all words, based on their word vectors\n",
    "- Plot the list of the top significant words in 2D. Next to each word output its text. The color of each point should indicate the genre for which it is most significant.\n",
    "\n",
    "You might prefer to use a different number of points or a slightly different methodology for improved results.  \n",
    "Analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeH5sXPdzpEi"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pt4DyJ1YzpEl"
   },
   "source": [
    "## Text Classification\n",
    "In this section, you'll build a text classifier, determining the genre of a song based on its lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSasg-B6zpEl"
   },
   "source": [
    "### Text classification using Bag-of-Words\n",
    "Build a Naive Bayes classifier based on the bag of Words.  \n",
    "You will need to divide your dataset into a train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7QvAxpZzpEl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrDoD8yrzpEn"
   },
   "source": [
    "Show the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96zWfIa0zpEo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "slJMMAcEzpEr"
   },
   "source": [
    "Show the classification report - precision, recall, f1 for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBa1iP84zpEs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qgl3kUfFzpEw"
   },
   "source": [
    "### Text classification using Word Vectors\n",
    "#### Average word vectors\n",
    "Do the same, using a classifier that averages the word vectors of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJ43fpl3zpEx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1NynAZazpEz"
   },
   "source": [
    "#### TfIdf Weighting\n",
    "Do the same, using a classifier that averages the word vectors of words in the document, weighting each word by its TfIdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77csqNKwzpEz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MdY18JFzpE1"
   },
   "source": [
    "### Text classification using ConvNet\n",
    "Do the same, using a ConvNet.  \n",
    "The ConvNet should get as input a 2D matrix where each column is an embedding vector of a single word, and words are in order. Use zero padding so that all matrices have a similar length.  \n",
    "Some songs might be very long. Trim them so you keep a maximum of 128 words (after cleaning stop words and rare words).  \n",
    "Initialize the embedding layer using the word vectors that you've trained before, but allow them to change during training.  \n",
    "\n",
    "Extra: Try training the ConvNet with 2 slight modifications:\n",
    "1. freezing the the weights trained using Word2vec (preventing it from updating)\n",
    "1. random initialization of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5S6vPLVzpE1"
   },
   "source": [
    "You are encouraged to try this question on your own.  \n",
    "\n",
    "You might prefer to get ideas from the paper \"Convolutional Neural Networks for Sentence Classification\" (Kim 2014, [link](https://arxiv.org/abs/1408.5882)).\n",
    "\n",
    "There are several implementations of the paper code in PyTorch online (see for example [this repo](https://github.com/prakashpandey9/Text-Classification-Pytorch) for a PyTorch implementation of CNN and other architectures for text classification). If you get stuck, they might provide you with a reference for your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1A0pfDYzpE1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "DL_word_embedding_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
